{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 와 MCP\n",
    "\n",
    "## Agent\n",
    "  * use LLMs to generate ‘thoughts’ bases on ‘observations’ to perform ‘actions’\n",
    "\n",
    "### [Agent 역할](https://huggingface.co/learn/agents-course/unit1/what-are-agents)\n",
    "  * 자연어 이해\n",
    "    * 인간의 지시 해석/응답\n",
    "  * 추론 및 계획\n",
    "    * 정보 분석, 결정, 문제 해결 전략 수립\n",
    "  * 툴과 상호작용\n",
    "    * 여러 툴을 이용해 정보 수집, 행동, 그 행동의 결과 관찰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구성\n",
    "\n",
    "#### [LLMs \\(Model\\)](https://huggingface.co/learn/agents-course/unit1/what-are-llms) \n",
    "\n",
    "* Agent 뇌 - 추론, 계획, 의사 결정\n",
    "* 인간의 언어 이해/생성\n",
    "* Objective\n",
    "  * EOS 전까지 이전의 토큰을 기반으로 다음 토큰 예측\n",
    "* Prompting the LLM\n",
    "  * 다음 토큰을 예측할때 주어지는 가이드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-mcp-adapters langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Tools](https://huggingface.co/learn/agents-course/unit1/tools)\n",
    "\n",
    "* Agent 몸 - 행동 실행\n",
    "* LLM은 외부 데이터나 연산을 할 수 없음\n",
    "* LLM 에게 주어지는 **함수**\n",
    "  * 도구의 기능에 대한 텍스트 설명\n",
    "  * 호출 가능한 함수\n",
    "  * 입력 인자와 타입\n",
    "  * 출력과 타입\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\"\"\"\n",
    "Tool Name: calculator, Description: Multiply two integers., \n",
    "Arguments: a: int, b: int, Outputs: int\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NWS_API_BASE = \"https://api.weather.gov\"\n",
    "\n",
    "def make_nws_request(url: str) -> dict[str] | None:\n",
    "    \"\"\"Make a request to the NWS API with proper error handling.\"\"\"\n",
    "    headers = {\"User-Agent\": \"weather-app/1.0\", \"Accept\": \"application/geo+json\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "@tool\n",
    "def get_forecast(latitude: float, longitude: float) -> str:\n",
    "    \"\"\"Get weather forecast for a location.\n",
    "\n",
    "    Args:\n",
    "        latitude: Latitude of the location\n",
    "        longitude: Longitude of the location\n",
    "    \"\"\"\n",
    "    points_url = f\"{NWS_API_BASE}/points/{latitude},{longitude}\"\n",
    "    points_data = make_nws_request(points_url)\n",
    "\n",
    "    if not points_data:\n",
    "        return \"Unable to fetch forecast data for this location.\"\n",
    "\n",
    "    forecast_url = points_data[\"properties\"][\"forecast\"]\n",
    "    forecast_data = make_nws_request(forecast_url)\n",
    "\n",
    "    if not forecast_data:\n",
    "        return \"Unable to fetch detailed forecast.\"\n",
    "\n",
    "    periods = forecast_data[\"properties\"][\"periods\"]\n",
    "    forecasts = []\n",
    "    for period in periods[:1]:\n",
    "        forecast = (\n",
    "            f\"{period['name']}:\"\n",
    "            f\"Temperature: {period['temperature']}°{period['temperatureUnit']}\"\n",
    "            f\"Wind: {period['windSpeed']} {period['windDirection']}\"\n",
    "            f\"Forecast: {period['detailedForecast']}\"\n",
    "        )\n",
    "        forecasts.append(forecast)\n",
    "    return \"\\n---\\n\".join(forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_forecast]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * LLM & Tool 동작\n",
    "    * 사용자 질문 -> LLM 이 호출할 Tool / 입력 인자 생성 -> Tool 호출 / 출련 반환 -> LLM 응답 생성\n",
    "\n",
    "  * Tool 디자인\n",
    "    * 툴들을 정해놓고 원하는 툴을 선택\n",
    "    * 툴을 직접 만들어내는 툴을 호출\n",
    "\n",
    "  * MCP, Model Context Protocol\n",
    "    * Tool 반복해서 구현하는 대신 재사용할 수 있도록 도구 제공 방식을 표준화한 프로토콜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### [AI Agent Workflow](https://huggingface.co/learn/agents-course/unit1/agent-steps-and-structure)\n",
    "\n",
    "\n",
    "<img width=\"500\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/73984383-cfc1-4e10-8db8-869dbb1d4af5\" />\n",
    "\n",
    "  * Thought\n",
    "    * LLM이 다음 스텝을 결정\n",
    "  * Action\n",
    "    * Tool 실행\n",
    "  * Observation\n",
    "    * Tool 응답 반영\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(model, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "New York 날씨는?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_forecast (call_JtXwfDBPbHonB9ZpCMDor59o)\n",
      " Call ID: call_JtXwfDBPbHonB9ZpCMDor59o\n",
      "  Args:\n",
      "    latitude: 40.7128\n",
      "    longitude: -74.006\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_forecast\n",
      "\n",
      "Today:Temperature: 80°FWind: 10 to 23 mph SWForecast: A slight chance of rain showers before 8am, then a slight chance of showers and thunderstorms. Partly sunny, with a high near 80. Southwest wind 10 to 23 mph, with gusts as high as 39 mph. Chance of precipitation is 20%.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "뉴욕의 오늘 날씨는 다음과 같습니다:\n",
      "\n",
      "- **기온:** 약 80°F (약 27°C)\n",
      "- **바람:** 남서쪽에서 시속 10에서 23마일까지, 최대 돌풍은 시속 39마일까지\n",
      "- **날씨 예보:** 오전 8시 이전에 약간의 비 소나기가 올 가능성이 있으며, 이후 약간의 비와 뇌우가 있을 수 있습니다. 구름이 조금 낄 것으로 예상되며 강수 확률은 20%입니다.\n"
     ]
    }
   ],
   "source": [
    "for s in agent.stream({\"messages\": \"New York 날씨는?\"}, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### [Thought](https://huggingface.co/learn/agents-course/unit1/thoughts)\n",
    "  * LLM이 다음 스텝을 결정\n",
    "  * Agent 내부 추론/계획 프로세스 \n",
    "    * 현재까지 정보로 다음 행동 결정\n",
    "      * “사용자가 이전에 Python을 선호한다고 했으니, 예제를 Python으로 제공해야겠다.”\n",
    "    * 복잡한 문제를 작은 단계로 나누기\n",
    "      * “이 작업을 완료하려면 1) 데이터 수집, 2) 분석, 3) 보고서 작성의 단계가 필요하다.”\n",
    "    * 과거 관찰로 전략 수정\n",
    "      * “이전 접근 방식이 효과적이지 않았으니 다른 전략을 시도해야겠다.”\n",
    "  * ReAct Aproach\n",
    "    * Reasoning & Acting\n",
    "    * 행동하기 전에 단계별로 동작하도록 유도하는 프롬프트 기법\n",
    "      * 마지막에 ‘Let’s think hink step by step’ 추가\n",
    "      * 한번에 결론 내는 대신 계획을 만들고 하나씩 해결\n",
    "      * 모델 Fine-tuning 에 \"think before answering\" 추가하고 <think>와 </think> 구각 포함해서 학습 - Deepseek R1 / OpenAI's o1\n",
    "\n",
    "#### [Action](https://huggingface.co/learn/agents-course/unit1/actions)\n",
    "  * Tool 실행\n",
    "  * Agent 가 수행하는 구체적인 작업\n",
    "  * Stop and Parse Aproach\n",
    "    * 구조화된 형식으로 생성\n",
    "    * 불필요한 토큰 생성 중단\n",
    "    * 출력 중 호출할 도구와 파라미터만 파싱\n",
    "\n",
    "\n",
    "#### [Observation](https://huggingface.co/learn/agents-course/unit1/observations)\n",
    "  * Tool 응답 반영\n",
    "  * Agent Action으로 얻은 피드백\n",
    "    * 행동 성공 여부나 결과 데이터 피드백 수집\n",
    "    * 결과 통합\n",
    "    * 다음 생각 전략 조정\n",
    "\n",
    "    \n",
    "#### Thought-Action-Observation Cycle\n",
    "  * End or Another thought?\n",
    "  * 목표 달성 전까지 Cycle 반복\n",
    "  * 툴 활용, LLM이 정적인 지식을 넘어서 실시간 데이터 활용\n",
    "  * 동적 조절, 정확한 정답이 나올때 까지 진행\n",
    "\n",
    "#### 만약에\n",
    "* LLM 이 멍청하면? 정확한지 판단을 할 수 없음\n",
    "* Tool 이 부족하면? 행동할 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "뉴욕 날씨랑 서울 날씨 비교해줘. 서울 날씨 수집이 안되면 LA 랑 비교해\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_forecast (call_vp4Jik4vqfuhN0OaWg9KAwTe)\n",
      " Call ID: call_vp4Jik4vqfuhN0OaWg9KAwTe\n",
      "  Args:\n",
      "    latitude: 40.7128\n",
      "    longitude: -74.006\n",
      "  get_forecast (call_BrTNeEfwSbpyRVBZ4FMad63q)\n",
      " Call ID: call_BrTNeEfwSbpyRVBZ4FMad63q\n",
      "  Args:\n",
      "    latitude: 37.5665\n",
      "    longitude: 126.978\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_forecast\n",
      "\n",
      "Unable to fetch forecast data for this location.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_forecast (call_k6SKTliHZRE8OCoeLnAYFQgu)\n",
      " Call ID: call_k6SKTliHZRE8OCoeLnAYFQgu\n",
      "  Args:\n",
      "    latitude: 34.0522\n",
      "    longitude: -118.2437\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_forecast\n",
      "\n",
      "Overnight:Temperature: 53°FWind: 0 mph Forecast: Mostly clear, with a low around 53. North northwest wind around 0 mph.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "현재 날씨 정보를 비교해보면 다음과 같습니다:\n",
      "\n",
      "- **뉴욕 (New York):**\n",
      "  - 온도: 80°F\n",
      "  - 바람: 10에서 23 mph (서남풍)\n",
      "  - 날씨 예보: 오전 8시 이전에 약간의 비 샤워 가능성이 있으며, 그 이후로도 약간의 샤워와 천둥 번개 가능성이 있습니다. 대체로 흐림, 최고기온은 80°F입니다. 강수 확률은 20%입니다.\n",
      "\n",
      "- **로스앤젤레스 (Los Angeles):**\n",
      "  - 온도: 53°F\n",
      "  - 바람: 0 mph (북북서풍)\n",
      "  - 날씨 예보: 대체로 맑으며, 최저기온은 약 53°F입니다.\n",
      "\n",
      "서울 날씨 정보를 가져올 수 없어서 LA와 비교했습니다. 뉴욕은 비 가능성이 약간 있으며 바람이 비교적 강한 편이고, LA는 맑고 바람이 거의 없는 조용한 날씨입니다.\n"
     ]
    }
   ],
   "source": [
    "for s in agent.stream({\"messages\": \"뉴욕 날씨랑 서울 날씨 비교해줘. 서울 날씨 수집이 안되면 LA 랑 비교해\"}, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Agentic System](https://www.anthropic.com/engineering/building-effective-agents)\n",
    "  * Workflow\n",
    "    * LLM 과 Tool 이 이미 정의된 방식으로 동작\n",
    "    * ex) RAG\n",
    "      * 쿼리 기반 DB 검색 후 요약 (싱글 스텝)\n",
    "  * Agent\n",
    "    * LLM 이 직접 목표 달성을 위해 어떤 플로우로 Tool 을 사용할 것인지 지시\n",
    "    * ex) Agnetic RAG\n",
    "      * 쿼리 기반 DB 검색 후 정보가 부족하면 다른 쿼리로 재질의 후 요약\n",
    "      * Query Reformulation, Result Validation, Multi-Step Retrieval, Source Integration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MCP](https://modelcontextprotocol.io/introduction)\n",
    "\n",
    "* Tool 반복해서 구현하는 대신 재사용할 수 있도록 도구 제공 방식을 표준화한 프로토콜 ?\n",
    "\n",
    "\n",
    "* Model Context Protocol\n",
    "  * Model: LLM\n",
    "  * Context: 에이전트가 작업을 수행할 때 참고하는 정보\n",
    "  * Protocol: 의사소통 규칙\n",
    "\n",
    "\n",
    "* LLM 과 데이터, Tool 간 통합을 표준화하는 개방형 프로토콜 (USB-C)\n",
    "  * Resources, Prompts, Tools\n",
    "\n",
    "* MCP 역할\n",
    "  * 컨텍스트 공유\n",
    "  * 모듈 간 일관성 유지\n",
    "  * 의사결정 흐름 정렬\n",
    "\n",
    "\n",
    "* MCP Architecture\n",
    "  * MCP Host\n",
    "    * Claude Desktop, Cursor\n",
    "    * MCP 를 통해 데이터에 접근하는 프로그램\n",
    "  * MCP Server\n",
    "    * MCP 를 통해 기능을 노출하는 프로그램\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Server Concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"weather\")\n",
    "\n",
    "if __name__ == \"__main___\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* LLM 이 읽을 수 있는 데이터 정보 노출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.resource(\"file:///logs/app.log\") # log files, images, database, ...\n",
    "def read_log_file() -> str:\n",
    "    \"\"\"Read Log file\"\"\"\n",
    "    return \"Log File ~~~\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promts\n",
    "\n",
    "* LLM Intercation 패턴 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp.prompts import base\n",
    "\n",
    "@mcp.prompt()\n",
    "def review_code(code: str) -> str:\n",
    "    return f\"Please review this code:\\n\\n{code}\"\n",
    "\n",
    "\n",
    "@mcp.prompt()\n",
    "def debug_error(error: str) -> list[base.Message]:\n",
    "    return [\n",
    "        base.UserMessage(\"I'm seeing this error:\"),\n",
    "        base.UserMessage(error),\n",
    "        base.AssistantMessage(\"I'll help debug that. What have you tried so far?\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "* 외부 작업을 수행할 수 있게 하는 기능 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "@mcp.tool()\n",
    "async def fetch_weather(city: str) -> str:\n",
    "    \"\"\"Fetch current weather for a city\"\"\"\n",
    "    response = requests.get(f\"https://api.weather.com/{city}\")\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Best Practice](https://modelcontextprotocol.io/tutorials/building-mcp-with-llms)\n",
    "\n",
    "* 복잡한 기능은 나눠서 구현\n",
    "* 각 컴포넌트별 테스트 철저히\n",
    "* 보안 고려: 입력 검증 및 접근 제한\n",
    "* 코드 문서화\n",
    "* MCP 스펙에 맞게 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP 설정\n",
    "* ```python \n",
    "    # server.py\n",
    "\n",
    "    from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "    mcp = FastMCP(\"weather\")\n",
    "\n",
    "    mcp.tool(get_forecast)\n",
    "    mcp.run(transport=\"sse\") # \"stdio\"\n",
    "    ```\n",
    "\n",
    "### MCP Server 실행\n",
    "* ```bash\n",
    "    python server.py\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCP Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Cursor\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"server-name-stdio\": {\n",
    "      \"command\": \"python\",\n",
    "      \"args\": [\"server.py\"]\n",
    "    },\n",
    "    \"server-name-sse\": {\n",
    "      \"url\": \"http://localhost:3000/sse\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in sse_reader: peer closed connection without sending complete message body (incomplete chunked read)\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient({\n",
    "    \"server-name-sse\": {\n",
    "        \"url\": \"http://localhost:8000/sse\",\n",
    "        \"transport\": \"sse\",\n",
    "    }\n",
    "})\n",
    "_ = await client.__aenter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='get_forecast', description='Get weather forecast for a location.\\n\\n    Args:\\n        latitude: Latitude of the location\\n        longitude: Longitude of the location\\n    ', args_schema={'properties': {'latitude': {'title': 'Latitude', 'type': 'number'}, 'longitude': {'title': 'Longitude', 'type': 'number'}}, 'required': ['latitude', 'longitude'], 'title': 'get_forecastArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10c8fbce0>)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tools = client.get_tools()\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(model, client.get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "뉴욕 날씨 알려줘\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_forecast (call_iNBEdgb81u4ukMQKGsmN1cTn)\n",
      " Call ID: call_iNBEdgb81u4ukMQKGsmN1cTn\n",
      "  Args:\n",
      "    latitude: 40.7128\n",
      "    longitude: -74.006\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_forecast\n",
      "\n",
      "Today:Temperature: 80°FWind: 10 to 23 mph SWForecast: A slight chance of rain showers before 8am, then a slight chance of showers and thunderstorms. Partly sunny, with a high near 80. Southwest wind 10 to 23 mph, with gusts as high as 39 mph. Chance of precipitation is 20%.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "오늘 뉴욕의 날씨는 다음과 같습니다:\n",
      "\n",
      "- **온도:** 80°F\n",
      "- **바람:** 서남서 방향으로 10에서 23 mph 속도의 바람이 붑니다.\n",
      "- **날씨 예보:** 오늘 오전 8시 전에는 비가 약간 올 가능성이 있으며, 그 후에는 소나기와 천둥번개가 약간 있을 수 있습니다. 대체로 맑음이며, 최고 기온은 약 80°F입니다. 바람은 서남서 방향으로 최대 39 mph의 강풍이 불 수 있습니다. 강수 확률은 20%입니다.\n"
     ]
    }
   ],
   "source": [
    "async for s in agent.astream({\"messages\": \"뉴욕 날씨 알려줘\"}, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MCP Official Servers](https://github.com/modelcontextprotocol/servers)\n",
    "\n",
    "* [Github](https://github.com/modelcontextprotocol/servers/tree/main/src/github)\n",
    "* [ElasticSearch](https://github.com/elastic/mcp-server-elasticsearch)\n",
    "* [Notion](https://github.com/makenotion/notion-mcp-server#readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
